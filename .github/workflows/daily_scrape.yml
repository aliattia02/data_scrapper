# .github/workflows/daily_scrape.yml
# Automated daily scraping at 6 AM Egypt time

name: ğŸ‡ªğŸ‡¬ Daily Grocery Scraping

on:
  schedule:
    # Runs at 6 AM Egypt time (4 AM UTC)
    - cron: '0 4 * * *'

  workflow_dispatch:  # Manual trigger
    inputs:
      store:
        description: 'Store to scrape (all, carrefour, metro, or kazyon)'
        required: false
        default: 'all'

jobs:
  scrape-and-export:
    runs-on: ubuntu-latest

    steps:
      - name: ğŸ“¥ Checkout repository
        uses: actions/checkout@v3

      - name: ğŸ Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: ğŸ“¦ Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          
          # Install Tesseract OCR
          sudo apt-get update
          sudo apt-get install -y tesseract-ocr tesseract-ocr-ara
          
          # Install Chrome for Selenium
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list'
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable
          
          # Install ChromeDriver
          wget -N https://chromedriver.storage.googleapis.com/LATEST_RELEASE
          CHROMEDRIVER_VERSION=$(cat LATEST_RELEASE)
          wget -N https://chromedriver.storage.googleapis.com/${CHROMEDRIVER_VERSION}/chromedriver_linux64.zip
          unzip -o chromedriver_linux64.zip
          sudo mv chromedriver /usr/local/bin/
          sudo chmod +x /usr/local/bin/chromedriver

      - name: ğŸ—„ï¸ Initialize database
        run: |
          python -m src.main db init

      - name: ğŸ›’ Run scrapers
        run: |
          STORE="${{ github.event.inputs.store || 'all' }}"
          python -m src.main scrape --store $STORE
        continue-on-error: true

      - name: ğŸ“¸ Process Kazyon flyers (if available)
        run: |
          if [ -d "data/flyers" ] && [ "$(ls -A data/flyers)" ]; then
            python -m src.main ocr --batch ./data/flyers/
          else
            echo "No flyers to process"
          fi
        continue-on-error: true

      - name: ğŸ“¦ Export data
        run: |
          python -m src.main export --format json
          python -m src.main export --format csv

      - name: ğŸ“Š Generate stats
        run: |
          python -c "
          from src.database.manager import DatabaseManager
          from src.database.models import Product
          from sqlalchemy import func
          
          db = DatabaseManager()
          session = db.get_session()
          
          total = session.query(func.count(Product.id)).scalar()
          by_store = session.query(Product.store, func.count(Product.id)).group_by(Product.store).all()
          
          print(f'ğŸ“ˆ Scraping Summary:')
          print(f'Total products: {total}')
          for store, count in by_store:
              print(f'  {store}: {count}')
          
          session.close()
          "

      - name: ğŸ’¾ Upload artifacts
        uses: actions/upload-artifact@v3
        with:
          name: scraped-data-${{ github.run_number }}
          path: |
            data/exports/*.json
            data/exports/*.csv
            data/database/products.db
          retention-days: 30

      - name: ğŸš€ Commit and push data (optional)
        if: success()
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          git add data/exports/*.json || true
          git add data/exports/*.csv || true
          
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "ğŸ¤– Automated scraping $(date +'%Y-%m-%d %H:%M')"
            git push
          fi
        continue-on-error: true

      - name: ğŸ“§ Send notification on failure
        if: failure()
        run: |
          echo "âŒ Scraping job failed. Check logs for details."
          # Add notification service here (email, Slack, Discord, etc.)

  # Optional: Deploy API
  deploy-api:
    needs: scrape-and-export
    runs-on: ubuntu-latest
    if: success()

    steps:
      - name: ğŸš€ Deploy API to production
        run: |
          echo "Deploy API to your hosting service"
          # Add deployment steps here (Railway, Heroku, AWS, etc.)